{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a05262-6dca-47de-aa29-352737a9ec7b",
   "metadata": {},
   "source": [
    "<h2>1.Derive an analytical solution to the regression problem.</h2>  \n",
    "$ y = w_1x_1 + ... w_nx_n + w_0$  \n",
    "regression problem is find a mapping function f from X to Y.  <br>\n",
    "$$f : X -> y | y \\in R, X \\in R^D$$  <br>\n",
    "and i want to model the dependency between y and x like  <br>\n",
    " $$y = f(x)$$  \n",
    "$$ y = w_1x_1 + ... w_nx_n + w_0$$ \n",
    "$$ f(x) = <w_i, x_i> + w_0 $$  \n",
    "where $w$ - is slope   \n",
    "$w_0$ - bais   \n",
    "X - input vector <br>\n",
    "i made convert   <br>\n",
    "$$\n",
    "(x_{i1} \\; \\ldots \\; x_{iD}) \\cdot\n",
    "\\begin{pmatrix}\n",
    "w_1 \\\\ \\vdots \\\\ w_D\n",
    "\\end{pmatrix}\n",
    "+ w_0 =\n",
    "(1 \\; x_{i1} \\; \\ldots \\; x_{iD}) \\cdot\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D\n",
    "\\end{pmatrix}\n",
    "$$  <br>\n",
    "$ f_w (x_i) = <w, x_i>$ <br>\n",
    "Let us take the Euclidean distance between the target vector and the model response vector as the loss function - L2.\n",
    "\n",
    "$$\n",
    "L(f, X, y) = \\| y - f(X) \\|_2^2 =\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\| y - Xw \\|_2^2 = \\sum_{i=1}^{N} \\left( y_i - \\langle w, x_i \\rangle \\right)^2\n",
    "$$\n",
    "and convert it to MSE - mean Squared Error\n",
    "$$\n",
    "MSE(f, X, y) = 1/N\\ || y - Xw \\||_2^2 = 1/N \\sum_{i=1}^{N} \\left( y_i - \\langle w, x_i \\rangle \\right)^2\n",
    "$$\n",
    "\n",
    "and the optimization task is\n",
    "$$\n",
    "|| y - Xw \\||_2^2 -> min\n",
    "$$\n",
    "\n",
    "Analytical solution can be obtained by projecting the target vector onto the subspace spanned by the features. Let the columns of the feature matrix $(X)$ be denoted as $( x^{(1)}, \\dots, x^{(D)})$. Then the regression problem can be formulated as follows:\n",
    "\n",
    "> Find a linear combination of the columns $( x^{(1)}, \\dots, x^{(D)})$ that best approximates the target vector $( y )$ in the Euclidean norm sense.\n",
    "\n",
    "This means we are looking for weights \\( w \\) such that the predicted values\n",
    "$$\n",
    "Xw = w_1 x^{(1)} + \\dots + w_D x^{(D)}\n",
    "$$\n",
    "are as close as possible to the vector \\( y \\). In other words, we want to find the projection $( y_{\\parallel} = Xw )$ onto the subspace spanned by the features so that the orthogonal component\n",
    "$$\n",
    "y_{\\perp} = y - Xw\n",
    "$$\n",
    "is perpendicular to each feature vector:\n",
    "$$\n",
    "y_{\\perp} \\perp x^{(1)}, \\dots, x^{(D)}.\n",
    "$$\n",
    "\n",
    "This orthogonality condition can be expressed in matrix form as\n",
    "$$\n",
    "X^T (y - Xw) = 0,\n",
    "$$\n",
    "where each equation corresponds to the condition that the residuals are orthogonal to a feature. <br>\n",
    "\n",
    "\n",
    "These equations are known as the **normal equations** for the least squares problem. By solving them, we obtain the analytical formula for the optimal weights:\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y,\n",
    "$$\n",
    "which is known as the **normal equation formula** or the analytical solution for linear regression.\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "error = \\sum (y - X W)^2 = (y - X W)^T (y - X W) = y^T y - y^T X W - (X W)^T y + (X W)^T (X W) = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "2 y^T X + 2 X^T X W = 0\n",
    "$$\n",
    "$$\n",
    "(X^T X) W = y^T X\n",
    "$$\n",
    "$$\n",
    "W = (X^T X)^{-1} y^T X\n",
    "$$\n",
    "\n",
    "> **Note.**  \n",
    "> - For the solution to exist, the matrix $( X^T X)$ must be non-singular (invertible), which requires the columns of $( X )$ to be linearly independent.\n",
    "> - The analytical solution is practical for small problems, but for large datasets, it is better to use numerical optimization methods (e.g., stochastic gradient descent), since computing the inverse $((X^T X)^{-1})$ becomes expensive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcc2c6b-c980-417f-9a42-1aa0e9b30ce6",
   "metadata": {},
   "source": [
    "<h2>2. What changes in the solution when L1 and L2 regularizations are added to the loss function?</h2>\n",
    "\n",
    " The idea of this addition is that the models do not try to learn complex patterns and try to find the solution. In other words, if the model starts to overfit in the process of finding a solution, then the addition in the in the error function will begin to increase, forcing the model to stop training.\n",
    "\n",
    "$$\n",
    "min_w\\sum_{i=1}^{N} \\left( L( y_i, f( w, x_i) \\right)\n",
    "$$\n",
    "\n",
    "![loss_without_regularization](https://raw.githubusercontent.com/ardipazij/s21_ML/main/ML-2/images/loss_without_regularization.png)\n",
    "\n",
    "$$\n",
    "min_w\\sum_{i=1}^{N} \\left( L( y_i, f( w, x_i) \\right) + \\alpha R(w)\n",
    "$$\n",
    "![loss_with_regularization](https://raw.githubusercontent.com/ardipazij/s21_ML/main/ML-2/images/loss_with_regularization.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1800456b-6aa5-4eca-b2e0-e79013de28d9",
   "metadata": {},
   "source": [
    "<h2>3. Explain why L1 regularization is often used to select features. Why are there many weights equal to 0 after the model is fit?</h2>\n",
    "\n",
    "When using L2 (Ridge), we add the sum of the squared weights:\n",
    "$$\n",
    "R(w) = \\sum w_i^2\n",
    "$$\n",
    "When updating the weight, we subtract its derivative (the derivative of the square is $2w$):\n",
    "$$\n",
    "w_{new} = w_{old} - \\eta \\cdot (\\frac{\\partial Loss}{\\partial w} + \\lambda \\cdot 2w)\n",
    "$$\n",
    "If the weight is large, the penalty will also be large. But as the weight decreases, the penalty decreases as well. It follows that the closer the weight is to zero, the weaker the effect of regularization becomes, which is why complete zeroing does not occur.When using L1 regularization, we add the sum of the absolute values of the weights:\n",
    "$$\n",
    "R(w) = \\sum |w_i|\n",
    "$$\n",
    "The derivative of the absolute value is either $+1$ (if $w > 0$) or $-1$ (if $w < 0$):\n",
    "$$\n",
    "w_{new} = w_{old} - \\eta \\cdot (\\frac{\\partial Loss}{\\partial w} + \\lambda \\cdot \\text{sign}(w))\n",
    "$$\n",
    "As the error gradually decreases during optimization, the penalty remains the same. Regularization continues to reduce the weight with the same intense force until it crosses zero. At this point, gradient descent simply stops it at the 0 mark.\n",
    "\n",
    "The application of L1 regularisation leads to features that do not have a significant impact on the response being given a weight of 0 as a result of optimisation.\n",
    "This allows features that have a weak influence on the target to be conveniently removed. In addition, it makes it possible to automatically eliminate features that participate in approximate linear relationships, thus avoiding problems associated with multicollinearity (sensitivity to small changes in data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb150c1-3817-4e5d-b426-02c42dbc7119",
   "metadata": {},
   "source": [
    "<h2>4. Explain how you can use the same models (Linear regression, Ridge, etc.) but make it possible to fit nonlinear dependencies.</h2>\n",
    "\n",
    "If we have non-linear features, we can create new features that will depend on the non-linear feature.\n",
    "\n",
    "![feature_engineering](https://raw.githubusercontent.com/ardipazij/s21_ML/main/ML-2/images/feature_engineering.png)\n",
    "\n",
    "\n",
    "Then, if we have two variables $( X_1 )$ and $( X_2 )$, we can add a term like $( \\sin(x) )$.  \n",
    "As a result, a two-dimensional nonlinear problem is transformed into a three-dimensional linear regression.\n",
    "\n",
    "We can add all kinds of different features, and by having more data, we can describe complex relationships in a more convenient way.  \n",
    "The process of finding such additional features is called **feature engineering**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e045819-c09a-49bc-a384-cac8652c93c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WSL Python 3.10 s21_ml_projects",
   "language": "python",
   "name": "s21_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
